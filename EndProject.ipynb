{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":559659,"sourceType":"datasetVersion","datasetId":263563},{"sourceId":8150530,"sourceType":"datasetVersion","datasetId":4820390},{"sourceId":8422267,"sourceType":"datasetVersion","datasetId":4841567},{"sourceId":8723500,"sourceType":"datasetVersion","datasetId":4841471}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Resources: \n- https://www.mdpi.com/2073-4395/13/6/1633\n- http://places2.csail.mit.edu/results2016.html\n- https://www.researchgate.net/figure/Accuracy-of-ResNet-50-101-and-152-on-ImageNet-and-CIFAR-10-datasets-before-and-after_tbl3_373753811\n- https://github.com/DeepLabCut/DeepLabCut/wiki/What-neural-network-should-I-use%3F-(Trade-offs,-speed-performance,-and-considerations)\n- https://medium.com/@muhabd51/comparison-and-architecture-of-pre-trained-model-vgg-16-vgg-19-resnet-googlenet-alexnet-0fb459fff368\n- https://github.com/jcjohnson/cnn-benchmarks\n- https://arxiv.org/abs/1512.03385\n- https://medium.com/@enrico.randellini/image-classification-resnet-vs-efficientnet-vs-efficientnet-v2-vs-compact-convolutional-c205838bbf49","metadata":{}},{"cell_type":"markdown","source":"## Class classification","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport pandas as pd\nimport time\n\ndef get_class_name(class_number):\n    # Assume that the DataFrame 'data' is already defined and contains the data from 'files.csv'\n    data = pd.read_csv('/kaggle/input/images256/files.csv')\n\n    # Filter the data to get the rows with the given class number\n    filtered_data = data[data['class'] == class_number]\n    \n    # Extract the name after the first slash from the file_name column\n    names = filtered_data['file_name'].apply(lambda x: x.split('/')[1])\n    \n    # Return the first name in the list\n    return names.iloc[0] if len(names) > 0 else None\n\n# Function to load the model for inference\ndef load_model_for_inference(model_path, num_classes):\n    # Automatically detect the device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using {'GPU' if device.type == 'cuda' else 'CPU'} for inference\")\n\n    # Initialize the model with ResNet50\n    model = models.resnet50(pretrained=False)\n    # Update the last fully connected layer to match the number of classes\n    \n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n#     model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n#     model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n    \n    # Load the saved state dictionary\n    state_dict = torch.load(model_path, map_location=device)\n    \n    # Check if the model was saved with a DataParallel wrapper\n    if 'module.' in list(state_dict.keys())[0]:  # If the first key has 'module.', others will too\n        # If yes, add a DataParallel wrapper to the model\n        model = nn.DataParallel(model)\n    \n    # Move the model to the specified device\n    model = model.to(device)\n    \n    # Load the state dictionary into the model\n    model.load_state_dict(state_dict)\n    \n    model.eval()\n    return model\n\n# Function to preprocess the image\ndef preprocess_image(image_path):\n    # Open the image file\n    with Image.open(image_path) as img:\n        # Check if the image is already the right size\n        if img.size != (256, 256):\n            # If not, resize the image\n            img = img.resize((256, 256))\n        # Convert the image to RGB\n        img = img.convert('RGB')\n        # Define the transformation\n        transform = transforms.Compose([\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ])\n        # Apply the transformation\n        img_t = transform(img)\n        return img_t.unsqueeze(0)\n\n# Function to predict the class of an image\ndef predict_image(model, image):\n    # Automatically detect the device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    # Move the image to the same device as the model\n    image = image.to(device)\n    outputs = model(image)\n    _, predicted = torch.max(outputs, 1)\n    return predicted.item()\n\n# # Function to show the image and the prediction\n# def show_prediction(image_path, model_path, num_classes):\n#     model = load_model_for_inference(model_path, num_classes)\n#     image = preprocess_image(image_path)\n#     prediction = predict_image(model, image)\n#     plt.imshow(Image.open(image_path))\n#     plt.title(f'Predicted class: {get_class_name(prediction)}')\n#     plt.show()\n\n# Function to show the image and the prediction with timing\ndef show_prediction_with_timing(image_path, model_path, num_classes):\n    start_time = time.time()  # Start timing\n    \n    model = load_model_for_inference(model_path, num_classes)\n    image = preprocess_image(image_path)\n    prediction = predict_image(model, image)\n    \n    end_time = time.time()  # End timing\n    total_time = end_time - start_time  # Calculate total duration\n    \n    plt.imshow(Image.open(image_path))\n    plt.title(f'Predicted class: {get_class_name(prediction)}')\n    plt.show()\n    \n    print(f\"Total time taken for prediction: {total_time:.2f} seconds\")\n\n# Example\nimage_path = '/kaggle/input/test-images/Ontario-International-Airport.webp'\nmodel_path = '/kaggle/input/resnet-model/resnet50_best_model (1).pth'\nnum_classes = 205  \n\n\n# show_prediction(image_path, model_path, num_classes)\nshow_prediction_with_timing(image_path, model_path, num_classes)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport pandas as pd\nimport time\n\ndef get_class_name(class_number):\n    # Assume that the DataFrame 'data' is already defined and contains the data from 'files.csv'\n    data = pd.read_csv('/kaggle/input/images256/files.csv')\n\n    # Filter the data to get the rows with the given class number\n    filtered_data = data[data['class'] == class_number]\n    \n    # Extract the name after the first slash from the file_name column\n    names = filtered_data['file_name'].apply(lambda x: x.split('/')[1])\n    \n    # Return the first name in the list\n    return names.iloc[0] if len(names) > 0 else None\n\n# Function to load the model for inference\ndef load_model_for_inference(model_path, num_classes):\n    # Automatically detect the device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using {'GPU' if device.type == 'cuda' else 'CPU'} for inference\")\n\n    # Initialize the model with ResNet50\n    model = models.resnet50(pretrained=False)\n    # Update the last fully connected layer to match the number of classes\n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n    \n    # Load the saved state dictionary\n    state_dict = torch.load(model_path, map_location=device)\n    \n    # Check if the model was saved with a DataParallel wrapper\n    if 'module.' in list(state_dict.keys())[0]:  \n        model = nn.DataParallel(model)\n    \n    # Move the model to the specified device\n    model = model.to(device)\n    \n    # Load the state dictionary into the model\n    model.load_state_dict(state_dict)\n    \n    model.eval()\n    return model\n\n# Function to preprocess the image\ndef preprocess_image(image_path):\n    # Open the image file\n    with Image.open(image_path) as img:\n        # Check if the image is already the right size\n        if img.size != (256, 256):\n            # If not, resize the image\n            img = img.resize((256, 256))\n        # Convert the image to RGB\n        img = img.convert('RGB')\n        # Define the transformation\n        transform = transforms.Compose([\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ])\n        # Apply the transformation\n        img_t = transform(img)\n        return img_t.unsqueeze(0)\n\n# Function to test the prediction speed without loading the image\ndef test_prediction_speed(model, image, num_tests=100):\n    # Automatically detect the device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Move the image to the same device as the model\n    image = image.to(device)\n    \n    # Warm-up run to ensure fair timing\n    _ = model(image)\n    \n    # Start timing\n    start_time = time.time()\n    \n    # Run the prediction 'num_tests' times\n    for _ in range(num_tests):\n        _ = model(image)\n    \n    # End timing\n    end_time = time.time()\n    \n    # Calculate total and average duration\n    total_time = end_time - start_time\n    average_time = total_time / num_tests\n    \n    print(f\"Average prediction time over {num_tests} tests: {average_time:.5f} seconds\")\n\n# Example\nimage_path = '/kaggle/input/test-images/Ontario-International-Airport.webp'\nmodel_path = '/kaggle/input/resnet-model/resnet50_best_model.pth'\nnum_classes = 205\n\n# Load the model and preprocess the image once\nmodel = load_model_for_inference(model_path, num_classes)\nimage = preprocess_image(image_path)\n\n# Test the prediction speed\ntest_prediction_speed(model, image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install pretrainedmodels","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### all scenes models training","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom PIL import Image\nfrom torch.cuda.amp import GradScaler, autocast\nimport multiprocessing\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport time\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\n# Seed setting function\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# Set a seed for reproducibility\nseed = 42\nset_seed(seed)\n\nos.makedirs('./models/', exist_ok=True)\n\n# Get the number of available CPU cores\nnum_cores = multiprocessing.cpu_count()\n\n# Set the device to GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Define transformations for the training dataset with data augmentation\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n    transforms.RandomRotation(20),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Define transformations for the validation and test datasets without data augmentation\nval_test_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Custom dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None):\n        self.annotations = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n        image = Image.open(img_path).convert('RGB')\n        label = int(self.annotations.iloc[index, 1])\n\n        if self.transform:\n            image = self.transform(image)\n\n        return (image, label)\n\n# Load the dataset\ncsv_file = '/kaggle/input/images256/files.csv'\nroot_dir = '/kaggle/input/images256/'\ndataset = CustomDataset(csv_file=csv_file, root_dir=root_dir, transform=train_transform)\n\n# Split the dataset into training, validation, and test sets\ntrain_size = int(0.7 * len(dataset))\nval_size = int(0.15 * len(dataset))\ntest_size = len(dataset) - train_size - val_size\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(seed))\n\n# Update the transform of the validation and test datasets\nval_dataset.dataset.transform = val_test_transform\ntest_dataset.dataset.transform = val_test_transform\n\n# Create data loaders with multiple workers\ndef worker_init_fn(worker_id):\n    np.random.seed(seed + worker_id)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=num_cores, pin_memory=True, worker_init_fn=worker_init_fn)\nval_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=num_cores, pin_memory=True, worker_init_fn=worker_init_fn)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=num_cores, pin_memory=True, worker_init_fn=worker_init_fn)\n\n# Initialize mixed precision scaling\nscaler = GradScaler()\n\n# Define a function to create models\ndef create_model(model_name, num_classes):\n    if model_name == 'resnet50':\n        model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n    elif model_name == 'resnet18':\n        model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n    elif model_name == 'efficientnet':\n        model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n    elif model_name == 'efficientnet_v2_s':\n        model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n    else:\n        raise ValueError(\"Invalid model name\")\n    return model\n\n# Define the list of models to be trained\nmodel_names = ['resnet50','resnet18','efficientnet','efficientnet_v2_s']\nnum_classes = len(set(dataset.annotations['class']))\n\n# Define a function to train and evaluate a model\ndef train_and_evaluate(model_name, num_epochs=20):\n    model = create_model(model_name, num_classes)\n    \n    # If multiple GPUs are available, wrap model with DataParallel\n    if torch.cuda.device_count() > 1:\n        print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n        model = nn.DataParallel(model)\n\n    model = model.to(device)\n    \n    # Define the loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n    scheduler = ReduceLROnPlateau(optimizer, 'min')\n\n    # Early stopping parameters\n    early_stopping_patience = 5\n    min_loss = float('inf')\n    losses_since_improvement = 0\n    best_model_path = f'./models/{model_name}_best_model.pth'\n    \n    epoch_losses = []\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        top1_acc = 0.0\n        top5_acc = 0.0\n        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'{model_name} Epoch {epoch+1}/{num_epochs}')\n        for batch_idx, (inputs, labels) in progress_bar:\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            \n            # Mixed precision training\n            with autocast():\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n            \n            # Scales loss. Calls backward() on scaled loss to create scaled gradients.\n            scaler.scale(loss).backward()\n            \n            # Unscales gradients and calls or skips optimizer.step()\n            scaler.step(optimizer)\n            \n            # Updates the scale for next iteration\n            scaler.update()\n            \n            running_loss += loss.item()\n            progress_bar.set_postfix(loss=running_loss/(batch_idx+1))\n            \n            # Calculate top-1 and top-5 accuracy\n            _, pred = outputs.topk(5, 1, True, True)\n            pred = pred.t()\n            correct = pred.eq(labels.view(1, -1).expand_as(pred))\n            top1_correct = correct[:1].view(-1).float().sum(0, keepdim=True)\n            top5_correct = correct[:5].reshape(-1).float().sum(0, keepdim=True)\n            top1_acc += top1_correct.item()\n            top5_acc += top5_correct.item()\n        \n        epoch_loss = running_loss / len(train_loader)\n        epoch_top1_acc = top1_acc / len(train_loader.dataset)\n        epoch_top5_acc = top5_acc / len(train_loader.dataset)\n        print(f'{model_name} Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}, Top-1 Accuracy: {epoch_top1_acc}, Top-5 Accuracy: {epoch_top5_acc}')\n        \n        # Append the epoch loss to our list\n        epoch_losses.append(epoch_loss)\n        \n        # Check for early stopping\n        if epoch_loss < min_loss:\n            min_loss = epoch_loss\n            losses_since_improvement = 0\n            # Save the best model\n            torch.save(model.state_dict(), best_model_path)\n        else:\n            losses_since_improvement += 1\n            if losses_since_improvement >= early_stopping_patience:\n                print(f'Stopping early at epoch {epoch+1}')\n                break\n        \n        # Save the model at every epoch\n        torch.save(model.state_dict(), f'./models/{model_name}_model_epoch_{epoch}.pth')\n    \n    # Validation phase\n    model.eval()\n    val_loss = 0.0\n    top1_acc = 0.0\n    top5_acc = 0.0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n            _, pred = outputs.topk(5, 1, True, True)\n            pred = pred.t()\n            correct = pred.eq(labels.view(1, -1).expand_as(pred))\n            top1_correct = correct[:1].view(-1).float().sum(0, keepdim=True)\n            top5_correct = correct[:5].reshape(-1).float().sum(0, keepdim=True)\n            top1_acc += top1_correct.item()\n            top5_acc += top5_correct.item()\n\n    val_loss /= len(val_loader)\n    val_top1_acc = top1_acc / len(val_loader.dataset)\n    val_top5_acc = top5_acc / len(val_loader.dataset)\n    print(f'{model_name} Validation Loss: {val_loss}, Top-1 Accuracy: {val_top1_acc}, Top-5 Accuracy: {val_top5_acc}')\n    \n    return model, val_loss, val_top1_acc, val_top5_acc, epoch_losses\n\n# Function to test a model\ndef test_model(model, model_name):\n    model.eval()\n    test_loss = 0.0\n    top1_acc = 0.0\n    top5_acc = 0.0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item()\n\n            _, pred = outputs.topk(5, 1, True, True)\n            pred = pred.t()\n            correct = pred.eq(labels.view(1, -1).expand_as(pred))\n            top1_correct = correct[:1].view(-1).float().sum(0, keepdim=True)\n            top5_correct = correct[:5].reshape(-1).float().sum(0, keepdim=True)\n            top1_acc += top1_correct.item()\n            top5_acc += top5_correct.item()\n\n    test_loss /= len(test_loader)\n    test_top1_acc = top1_acc / len(test_loader.dataset)\n    test_top5_acc = top5_acc / len(test_loader.dataset)\n    print(f'{model_name} Test Loss: {test_loss}, Top-1 Accuracy: {test_top1_acc}, Top-5 Accuracy: {test_top5_acc}')\n    \n    return test_loss, test_top1_acc, test_top5_acc\n\n# Train and evaluate each model\nmodel_results = []\nfor model_name in model_names:\n    print(f'Training and evaluating {model_name}')\n    start_time = time.time()\n    model, val_loss, val_top1_acc, val_top5_acc, epoch_losses = train_and_evaluate(model_name)\n    train_time = time.time() - start_time\n    test_loss, test_top1_acc, test_top5_acc = test_model(model, model_name)\n    inference_time = time.time() - start_time - train_time\n    model_results.append((model_name, val_loss, val_top1_acc, val_top5_acc, test_loss, test_top1_acc, test_top5_acc, train_time, inference_time, epoch_losses))\n\n# Compare models\nresults_df = pd.DataFrame(model_results, columns=['Model', 'Val_Loss', 'Val_Top1_Acc', 'Val_Top5_Acc', 'Test_Loss', 'Test_Top1_Acc', 'Test_Top5_Acc', 'Train_Time', 'Inference_Time', 'Epoch_Losses'])\nprint(results_df)\n\n# Plot comparison of models\nresults_df.plot(x='Model', y=['Val_Top1_Acc', 'Test_Top1_Acc'], kind='bar')\nplt.title('Model Comparison: Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Model')\nplt.show()\n\nresults_df.plot(x='Model', y=['Train_Time', 'Inference_Time'], kind='bar')\nplt.title('Model Comparison: Training and Inference Time')\nplt.ylabel('Time (seconds)')\nplt.xlabel('Model')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"!lscpu","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### VGG16","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom PIL import Image\nfrom torch.cuda.amp import GradScaler, autocast\nimport multiprocessing\nfrom tqdm import tqdm\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n# Seed setting function\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# Set a seed for reproducibility\nseed = 42\nset_seed(seed)\n\nos.makedirs('./models/', exist_ok=True)\n\n# Get the number of available CPU cores\nnum_cores = multiprocessing.cpu_count()\n\n# Set the device to GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Define transformations for the training dataset with data augmentation\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n    transforms.RandomRotation(20),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Define transformations for the validation and test datasets without data augmentation\nval_test_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Custom dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None):\n        self.annotations = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n        image = Image.open(img_path).convert('RGB')\n        label = int(self.annotations.iloc[index, 1])\n\n        if self.transform:\n            image = self.transform(image)\n\n        return (image, label)\n\n# Load the dataset\ncsv_file = '/kaggle/input/images256/files.csv'\nroot_dir = '/kaggle/input/images256/'\ndataset = CustomDataset(csv_file=csv_file, root_dir=root_dir, transform=train_transform)\n\n# Split the dataset into training, validation, and test sets\ntrain_size = int(0.7 * len(dataset))\nval_size = int(0.15 * len(dataset))\ntest_size = len(dataset) - train_size - val_size\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(seed))\n\n# Update the transform of the validation and test datasets\nval_dataset.dataset.transform = val_test_transform\ntest_dataset.dataset.transform = val_test_transform\n\n# Create data loaders with multiple workers\ndef worker_init_fn(worker_id):\n    np.random.seed(seed + worker_id)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=num_cores, pin_memory=True, worker_init_fn=worker_init_fn)\nval_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=num_cores, pin_memory=True, worker_init_fn=worker_init_fn)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=num_cores, pin_memory=True, worker_init_fn=worker_init_fn)\n\n# Initialize mixed precision scaling\nscaler = GradScaler()\n\n# Define a function to create the VGG-16 model\ndef create_model(num_classes):\n    model = models.vgg16(pretrained=False)\n    model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n    return model\n\nnum_classes = len(set(dataset.annotations['class']))\n\n# Define a function to train the VGG-16 model\ndef train_model(num_epochs=20):\n    model = create_model(num_classes)\n    \n    # If multiple GPUs are available, wrap model with DataParallel\n    if torch.cuda.device_count() > 1:\n        print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n        model = nn.DataParallel(model)\n\n    model = model.to(device)\n    \n    # Define the loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n    scheduler = ReduceLROnPlateau(optimizer, 'min')\n\n    # Early stopping parameters\n    early_stopping_patience = 5\n    min_loss = float('inf')\n    losses_since_improvement = 0\n    best_model_path = './models/vgg16_best_model.pth'\n    \n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'VGG-16 Epoch {epoch+1}/{num_epochs}')\n        for batch_idx, (inputs, labels) in progress_bar:\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            \n            # Mixed precision training\n            with autocast():\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n            \n            # Scales loss. Calls backward() on scaled loss to create scaled gradients.\n            scaler.scale(loss).backward()\n            \n            # Unscales gradients and calls or skips optimizer.step()\n            scaler.step(optimizer)\n            \n            # Updates the scale for next iteration\n            scaler.update()\n            \n            running_loss += loss.item()\n            progress_bar.set_postfix(loss=running_loss/(batch_idx+1))\n        \n        epoch_loss = running_loss / len(train_loader)\n        print(f'VGG-16 Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}')\n        \n        # Check for early stopping\n        if epoch_loss < min_loss:\n            min_loss = epoch_loss\n            losses_since_improvement = 0\n            # Save the best model\n            torch.save(model.state_dict(), best_model_path)\n        else:\n            losses_since_improvement += 1\n            if losses_since_improvement >= early_stopping_patience:\n                print(f'Stopping early at epoch {epoch+1}')\n                break\n        \n        # Save the model at every epoch\n        torch.save(model.state_dict(), f'./models/vgg16_model_epoch_{epoch}.pth')\n\n# Train the model\nprint('Training VGG-16')\ntrain_model()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-16T23:36:12.448637Z","iopub.execute_input":"2024-06-16T23:36:12.449017Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Training VGG-16\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Let's use 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"VGG-16 Epoch 1/20: 100%|██████████| 1303/1303 [12:05<00:00,  1.79it/s, loss=5.14]\n","output_type":"stream"},{"name":"stdout","text":"VGG-16 Epoch 1/20, Loss: 5.143630721618465\n","output_type":"stream"},{"name":"stderr","text":"VGG-16 Epoch 2/20: 100%|██████████| 1303/1303 [12:08<00:00,  1.79it/s, loss=4.73]\n","output_type":"stream"},{"name":"stdout","text":"VGG-16 Epoch 2/20, Loss: 4.732838140298839\n","output_type":"stream"},{"name":"stderr","text":"VGG-16 Epoch 3/20: 100%|██████████| 1303/1303 [12:10<00:00,  1.78it/s, loss=4.46]\n","output_type":"stream"},{"name":"stdout","text":"VGG-16 Epoch 3/20, Loss: 4.4599734560307045\n","output_type":"stream"},{"name":"stderr","text":"VGG-16 Epoch 4/20: 100%|██████████| 1303/1303 [12:10<00:00,  1.78it/s, loss=4.22]\n","output_type":"stream"},{"name":"stdout","text":"VGG-16 Epoch 4/20, Loss: 4.223946701995428\n","output_type":"stream"},{"name":"stderr","text":"VGG-16 Epoch 5/20: 100%|██████████| 1303/1303 [12:10<00:00,  1.78it/s, loss=4.02]\n","output_type":"stream"},{"name":"stdout","text":"VGG-16 Epoch 5/20, Loss: 4.024440361917339\n","output_type":"stream"},{"name":"stderr","text":"VGG-16 Epoch 6/20: 100%|██████████| 1303/1303 [12:09<00:00,  1.78it/s, loss=3.85]\n","output_type":"stream"},{"name":"stdout","text":"VGG-16 Epoch 6/20, Loss: 3.85366937058391\n","output_type":"stream"},{"name":"stderr","text":"VGG-16 Epoch 7/20: 100%|██████████| 1303/1303 [12:09<00:00,  1.79it/s, loss=3.72]\n","output_type":"stream"},{"name":"stdout","text":"VGG-16 Epoch 7/20, Loss: 3.716675462122615\n","output_type":"stream"},{"name":"stderr","text":"VGG-16 Epoch 8/20: 100%|██████████| 1303/1303 [12:09<00:00,  1.79it/s, loss=3.6] \n","output_type":"stream"},{"name":"stdout","text":"VGG-16 Epoch 8/20, Loss: 3.5954585432182893\n","output_type":"stream"},{"name":"stderr","text":"VGG-16 Epoch 9/20: 100%|██████████| 1303/1303 [12:08<00:00,  1.79it/s, loss=3.49]\n","output_type":"stream"},{"name":"stdout","text":"VGG-16 Epoch 9/20, Loss: 3.493280484872146\n","output_type":"stream"},{"name":"stderr","text":"VGG-16 Epoch 10/20: 100%|██████████| 1303/1303 [12:09<00:00,  1.79it/s, loss=3.39]\n","output_type":"stream"},{"name":"stdout","text":"VGG-16 Epoch 10/20, Loss: 3.390757596776381\n","output_type":"stream"},{"name":"stderr","text":"VGG-16 Epoch 11/20: 100%|██████████| 1303/1303 [12:08<00:00,  1.79it/s, loss=3.3]\n","output_type":"stream"},{"name":"stdout","text":"VGG-16 Epoch 11/20, Loss: 3.3010051208373135\n","output_type":"stream"},{"name":"stderr","text":"VGG-16 Epoch 12/20:  46%|████▌     | 593/1303 [05:32<06:35,  1.80it/s, loss=3.21]","output_type":"stream"}]},{"cell_type":"markdown","source":"VGG16","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom PIL import Image\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nimport numpy as np\nimport random\nimport multiprocessing\nimport time\n\n# Seed setting function\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# Set a seed for reproducibility\nseed = 45\nset_seed(seed)\n\n# Get the number of available CPU cores\nnum_cores = multiprocessing.cpu_count()\n\n# Set the device to GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Define transformations for the validation and test datasets without data augmentation\nval_test_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Custom dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None):\n        self.annotations = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n        image = Image.open(img_path).convert('RGB')\n        label = int(self.annotations.iloc[index, 1])\n\n        if self.transform:\n            image = self.transform(image)\n\n        return (image, label)\n\n# Load the dataset\ncsv_file = '/kaggle/input/images256/files.csv'\nroot_dir = '/kaggle/input/images256/'\ndataset = CustomDataset(csv_file=csv_file, root_dir=root_dir, transform=val_test_transform)\n\n# Split the dataset into training, validation, and test sets\ntrain_size = int(0.7 * len(dataset))\nval_size = int(0.15 * len(dataset))\ntest_size = len(dataset) - train_size - val_size\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(seed))\n\n# Update the transform of the validation and test datasets\nval_dataset.dataset.transform = val_test_transform\ntest_dataset.dataset.transform = val_test_transform\n\n# Create data loaders with multiple workers\ndef worker_init_fn(worker_id):\n    np.random.seed(seed + worker_id)\n\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=num_cores, pin_memory=True, worker_init_fn=worker_init_fn)\n\n# Define a function to create the VGG-16 model\ndef create_model(num_classes):\n    model = models.vgg16(pretrained=False)\n    model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n    return model\n\nnum_classes = len(set(dataset.annotations['class']))\n\n# Function to load the model\ndef load_model(model_path, num_classes):\n    model = create_model(num_classes)\n    state_dict = torch.load(model_path, map_location=device)\n    \n    # Remove 'module.' prefix if it exists\n    new_state_dict = {}\n    for k, v in state_dict.items():\n        name = k[7:] if k.startswith('module.') else k\n        new_state_dict[name] = v\n    \n    model.load_state_dict(new_state_dict)\n    model = model.to(device)\n    model.eval()\n    return model\n\n# Function to evaluate the model with additional metrics\ndef evaluate_model(model, test_loader):\n    y_true = []\n    y_pred = []\n    y_probs = []\n    start_time = time.time()\n    with torch.no_grad():\n        for inputs, labels in tqdm(test_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            probs = torch.nn.functional.softmax(outputs, dim=1)\n            _, preds = torch.max(outputs, 1)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(preds.cpu().numpy())\n            y_probs.extend(probs.cpu().numpy())\n    total_time = time.time() - start_time\n    n_images = len(y_true)\n    \n    top1_accuracy = accuracy_score(y_true, y_pred)\n    \n    # Calculate top-5 accuracy\n    y_true_np = np.array(y_true)\n    y_probs_np = np.array(y_probs)\n    top5_preds = np.argsort(y_probs_np, axis=1)[:, -5:]\n    top5_correct = np.any(top5_preds == y_true_np[:, None], axis=1)\n    top5_accuracy = np.mean(top5_correct)\n\n    # Calculate additional metrics\n    precision = precision_score(y_true, y_pred, average='weighted')\n    recall = recall_score(y_true, y_pred, average='weighted')\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    roc_auc = roc_auc_score(y_true, y_probs, multi_class='ovr')\n\n    return {\n        'top1_accuracy': top1_accuracy,\n        'top5_accuracy': top5_accuracy,\n        'total_time': total_time,\n        'n_images': n_images,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'roc_auc': roc_auc\n    }\n\n# Load the trained model\nmodel_path = '/kaggle/input/resnet-model/vgg16_model_epoch_19.pth'\nmodel = load_model(model_path, num_classes)\n\n# Evaluate the model on the test set\nmetrics = evaluate_model(model, test_loader)\nprint(f\"Top-1 Accuracy: {metrics['top1_accuracy']}\")\nprint(f\"Top-5 Accuracy: {metrics['top5_accuracy']}\")\nprint(f\"Precision: {metrics['precision']}\")\nprint(f\"Recall: {metrics['recall']}\")\nprint(f\"F1 Score: {metrics['f1']}\")\nprint(f\"ROC AUC: {metrics['roc_auc']}\")\nprint(f\"Total Time: {metrics['total_time']} seconds\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T14:35:19.056356Z","iopub.execute_input":"2024-06-17T14:35:19.056605Z","iopub.status.idle":"2024-06-17T14:38:31.672218Z","shell.execute_reply.started":"2024-06-17T14:35:19.056583Z","shell.execute_reply":"2024-06-17T14:38:31.671115Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n100%|██████████| 280/280 [03:01<00:00,  1.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Top-1 Accuracy: 0.40676352845665015\nTop-5 Accuracy: 0.7127739984882842\nPrecision: 0.4290929144574817\nRecall: 0.40676352845665015\nF1 Score: 0.3937512479270663\nROC AUC: 0.9732979357683694\nTotal Time: 181.87083315849304 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Indoor data models training","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom PIL import Image\nfrom torch.cuda.amp import GradScaler, autocast\nimport multiprocessing\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport time\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\n# Seed setting function\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# Set a seed for reproducibility\nseed = 42\nset_seed(seed)\n\nos.makedirs('./models/', exist_ok=True)\n\n# Get the number of available CPU cores\nnum_cores = multiprocessing.cpu_count()\n\n# Set the device to GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Define transformations for the training dataset with data augmentation\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n    transforms.RandomRotation(20),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Define transformations for the validation and test datasets without data augmentation\nval_test_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Custom dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None, indoor_classes=None):\n        self.annotations = pd.read_csv(csv_file)\n        if indoor_classes:\n            self.annotations = self.annotations[self.annotations['class_name'].isin(indoor_classes)]\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n        image = Image.open(img_path).convert('RGB')\n        label = int(self.annotations.iloc[index, 1])\n\n        if self.transform:\n            image = self.transform(image)\n\n        return (image, label)\n\n# Define indoor classes\nindoor_classes = [\n    'motel', 'hotel', 'kitchenette', 'coffee_shop', 'kitchen', 'television_studio', \n    'inn', 'classroom', 'parlor', 'dinette', 'laundromat', 'basilica', 'bookstore', \n    'restaurant', 'apartment_building', 'art_gallery', 'living_room', 'ballroom', \n    'engine_room', 'beauty_salon', 'music_studio', 'closet', 'attic', 'martial_arts_gym', \n    'basement', 'bowling_alley', 'hotel_room', 'shoe_shop', 'mansion', 'gift_shop', \n    'waiting_room', 'fire_station', 'reception', 'dining_room', 'clothing_store', \n    'abbey', 'gas_station', 'ice_skating_rink', 'nursery', 'cafeteria', 'art_studio', \n    'bakery', 'jail_cell', 'cockpit', 'restaurant_kitchen', 'conference_center', \n    'food_court', 'ice_cream_parlor', 'boxing_ring', 'train_station', 'bus_interior', \n    'auditorium', 'schoolhouse', 'bar', 'pantry', 'hospital', 'corridor', \n    'office_building', 'supermarket', 'galley', 'game_room', 'banquet_hall', 'shower', \n    'locker_room', 'airport_terminal', 'bedroom', 'subway_station', 'aquarium', \n    'museum', 'home_office', 'hospital_room', 'kindergarden_classroom', 'butchers_shop', \n    'candy_store', 'office', 'conference_room', 'assembly_line', 'dorm_room'\n]\n\n# Load the dataset\ncsv_file = '/kaggle/input/images256/files.csv'\nroot_dir = '/kaggle/input/images256/'\ndataset = CustomDataset(csv_file=csv_file, root_dir=root_dir, transform=train_transform, indoor_classes=indoor_classes)\n\n# Split the dataset into training, validation, and test sets\ntrain_size = int(0.7 * len(dataset))\nval_size = int(0.15 * len(dataset))\ntest_size = len(dataset) - train_size - val_size\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(seed))\n\n# Update the transform of the validation and test datasets\nval_dataset.dataset.transform = val_test_transform\ntest_dataset.dataset.transform = val_test_transform\n\n# Create data loaders with multiple workers\ndef worker_init_fn(worker_id):\n    np.random.seed(seed + worker_id)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=num_cores, pin_memory=True, worker_init_fn=worker_init_fn)\nval_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=num_cores, pin_memory=True, worker_init_fn=worker_init_fn)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=num_cores, pin_memory=True, worker_init_fn=worker_init_fn)\n\n# Initialize mixed precision scaling\nscaler = GradScaler()\n\n# Define a function to create models\ndef create_model(model_name, num_classes):\n    if model_name == 'resnet50':\n        model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n    elif model_name == 'resnet18':\n        model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n    elif model_name == 'efficientnet':\n        model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n    elif model_name == 'efficientnet_v2_s':\n        model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n    else:\n        raise ValueError(\"Invalid model name\")\n    return model\n\n# Define the list of models to be trained\nmodel_names = ['resnet50','resnet18','efficientnet','efficientnet_v2_s']\nnum_classes = len(set(dataset.annotations['class']))\n\n# Define a function to train and evaluate a model\ndef train_and_evaluate(model_name, num_epochs=20):\n    model = create_model(model_name, num_classes)\n    \n    # If multiple GPUs are available, wrap model with DataParallel\n    if torch.cuda.device_count() > 1:\n        print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n        model = nn.DataParallel(model)\n\n    model = model.to(device)\n    \n    # Define the loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n    scheduler = ReduceLROnPlateau(optimizer, 'min')\n\n    # Early stopping parameters\n    early_stopping_patience = 5\n    min_loss = float('inf')\n    losses_since_improvement = 0\n    best_model_path = f'./models/indoor_{model_name}_best_model.pth'\n    \n    epoch_losses = []\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        top1_acc = 0.0\n        top5_acc = 0.0\n        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'{model_name} Epoch {epoch+1}/{num_epochs}')\n        for batch_idx, (inputs, labels) in progress_bar:\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            \n            # Mixed precision training\n            with autocast():\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n            \n            # Scales loss. Calls backward() on scaled loss to create scaled gradients.\n            scaler.scale(loss).backward()\n            \n            # Unscales gradients and calls or skips optimizer.step()\n            scaler.step(optimizer)\n            \n            # Updates the scale for next iteration\n            scaler.update()\n            \n            running_loss += loss.item()\n            progress_bar.set_postfix(loss=running_loss/(batch_idx+1))\n            \n                        # Calculate top-1 and top-5 accuracy\n            _, pred = outputs.topk(5, 1, True, True)\n            pred = pred.t()\n            correct = pred.eq(labels.view(1, -1).expand_as(pred))\n            top1_correct = correct[:1].view(-1).float().sum(0, keepdim=True)\n            top5_correct = correct[:5].reshape(-1).float().sum(0, keepdim=True)\n            top1_acc += top1_correct.item()\n            top5_acc += top5_correct.item()\n        \n        epoch_loss = running_loss / len(train_loader)\n        epoch_top1_acc = top1_acc / len(train_loader.dataset)\n        epoch_top5_acc = top5_acc / len(train_loader.dataset)\n        print(f'{model_name} Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}, Top-1 Accuracy: {epoch_top1_acc}, Top-5 Accuracy: {epoch_top5_acc}')\n        \n        # Append the epoch loss to our list\n        epoch_losses.append(epoch_loss)\n        \n        # Check for early stopping\n        if epoch_loss < min_loss:\n            min_loss = epoch_loss\n            losses_since_improvement = 0\n            # Save the best model\n            torch.save(model.state_dict(), best_model_path)\n        else:\n            losses_since_improvement += 1\n            if losses_since_improvement >= early_stopping_patience:\n                print(f'Stopping early at epoch {epoch+1}')\n                break\n        \n        # Save the model at every second epoch\n        if (epoch + 1) % 2 == 0:\n            torch.save(model.state_dict(), f'./models/indoor_{model_name}_epoch_{epoch+1}.pth')\n    \n    # Validation phase\n    model.eval()\n    val_loss = 0.0\n    top1_acc = 0.0\n    top5_acc = 0.0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n            _, pred = outputs.topk(5, 1, True, True)\n            pred = pred.t()\n            correct = pred.eq(labels.view(1, -1).expand_as(pred))\n            top1_correct = correct[:1].view(-1).float().sum(0, keepdim=True)\n            top5_correct = correct[:5].reshape(-1).float().sum(0, keepdim=True)\n            top1_acc += top1_correct.item()\n            top5_acc += top5_correct.item()\n\n    val_loss /= len(val_loader)\n    val_top1_acc = top1_acc / len(val_loader.dataset)\n    val_top5_acc = top5_acc / len(val_loader.dataset)\n    print(f'{model_name} Validation Loss: {val_loss}, Top-1 Accuracy: {val_top1_acc}, Top-5 Accuracy: {val_top5_acc}')\n    \n    return model, val_loss, val_top1_acc, val_top5_acc, epoch_losses\n\n# Function to test a model\ndef test_model(model, model_name):\n    model.eval()\n    test_loss = 0.0\n    top1_acc = 0.0\n    top5_acc = 0.0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item()\n\n            _, pred = outputs.topk(5, 1, True, True)\n            pred = pred.t()\n            correct = pred.eq(labels.view(1, -1).expand_as(pred))\n            top1_correct = correct[:1].view(-1).float().sum(0, keepdim=True)\n            top5_correct = correct[:5].reshape(-1).float().sum(0, keepdim=True)\n            top1_acc += top1_correct.item()\n            top5_acc += top5_correct.item()\n\n    test_loss /= len(test_loader)\n    test_top1_acc = top1_acc / len(test_loader.dataset)\n    test_top5_acc = top5_acc / len(test_loader.dataset)\n    print(f'{model_name} Test Loss: {test_loss}, Top-1 Accuracy: {test_top1_acc}, Top-5 Accuracy: {test_top5_acc}')\n    \n    return test_loss, test_top1_acc, test_top5_acc\n\n# Train and evaluate each model\nmodel_results = []\nfor model_name in model_names:\n    print(f'Training and evaluating {model_name}')\n    start_time = time.time()\n    model, val_loss, val_top1_acc, val_top5_acc, epoch_losses = train_and_evaluate(model_name)\n    train_time = time.time() - start_time\n    test_loss, test_top1_acc, test_top5_acc = test_model(model, model_name)\n    inference_time = time.time() - start_time - train_time\n    model_results.append((model_name, val_loss, val_top1_acc, val_top5_acc, test_loss, test_top1_acc, test_top5_acc, train_time, inference_time, epoch_losses))\n\n# Compare models\nresults_df = pd.DataFrame(model_results, columns=['Model', 'Val_Loss', 'Val_Top1_Acc', 'Val_Top5_Acc', 'Test_Loss', 'Test_Top1_Acc', 'Test_Top5_Acc', 'Train_Time', 'Inference_Time', 'Epoch_Losses'])\nprint(results_df)\n\n# Plot comparison of models\nresults_df.plot(x='Model', y=['Val_Top1_Acc', 'Test_Top1_Acc'], kind='bar')\nplt.title('Model Comparison: Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Model')\nplt.show()\n\nresults_df.plot(x='Model', y=['Train_Time', 'Inference_Time'], kind='bar')\nplt.title('Model Comparison: Training and Inference Time')\nplt.ylabel('Time (seconds)')\nplt.xlabel('Model')\nplt.show()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\ndef print_directory_tree(root_dir, padding=\"\"):\n    print(padding[:-1] + \"+--\" + os.path.basename(root_dir))\n    padding = padding + \"    \"\n    for item in os.listdir(root_dir):\n        path = os.path.join(root_dir, item)\n        if os.path.isdir(path):\n            print_directory_tree(path, padding + \"│   \")\n\n# Specify the root directory\nroot_directory = '/kaggle/input/images256'\n\n# Print the directory tree starting from the root directory\nprint_directory_tree(root_directory)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Indoor scenes testing","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport random\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom PIL import Image\nimport multiprocessing\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom scipy.stats import friedmanchisquare\nimport scikit_posthocs as sp\nimport seaborn as sns\nimport psutil\nfrom pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetPowerUsage, nvmlShutdown\n\n# Set a seed for reproducibility in initial data splitting\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed = 42\nset_seed(seed)\n\n# Get the number of available CPU cores\nnum_cores = multiprocessing.cpu_count()\n\n# Set the device to GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Define transformations for the validation and test datasets without data augmentation\nval_test_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Custom dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None, indoor_classes=None):\n        self.annotations = pd.read_csv(csv_file)\n        self.annotations['class_name'] = self.annotations['file_name'].apply(lambda x: x.split('/')[1])\n        if indoor_classes:\n            self.annotations = self.annotations[self.annotations['class_name'].isin(indoor_classes)]\n        self.root_dir = root_dir\n        self.transform = transform\n\n        # Verify and set the number of classes\n        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(sorted(self.annotations['class_name'].unique()))}\n        self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}\n        self.num_classes = len(self.class_to_idx)\n        \n        # Update labels to class indices\n        self.annotations['class_idx'] = self.annotations['class_name'].map(self.class_to_idx)\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n        image = Image.open(img_path).convert('RGB')\n        label = int(self.annotations.iloc[index, self.annotations.columns.get_loc('class_idx')])\n\n        if self.transform:\n            image = self.transform(image)\n\n        return (image, label)\n\n# Define indoor classes\nindoor_classes = [\n    'motel', 'hotel', 'kitchenette', 'coffee_shop', 'kitchen', 'television_studio', \n    'inn', 'classroom', 'parlor', 'dinette', 'laundromat', 'basilica', 'bookstore', \n    'restaurant', 'apartment_building', 'art_gallery', 'living_room', 'ballroom', \n    'engine_room', 'beauty_salon', 'music_studio', 'closet', 'attic', 'martial_arts_gym', \n    'basement', 'bowling_alley', 'hotel_room', 'shoe_shop', 'mansion', 'gift_shop', \n    'waiting_room', 'fire_station', 'reception', 'dining_room', 'clothing_store', \n    'abbey', 'gas_station', 'ice_skating_rink', 'nursery', 'cafeteria', 'art_studio', \n    'bakery', 'jail_cell', 'cockpit', 'restaurant_kitchen', 'conference_center', \n    'food_court', 'ice_cream_parlor', 'boxing_ring', 'train_station', 'bus_interior', \n    'auditorium', 'schoolhouse', 'bar', 'pantry', 'hospital', 'corridor', \n    'office_building', 'supermarket', 'galley', 'game_room', 'banquet_hall', 'shower', \n    'locker_room', 'airport_terminal', 'bedroom', 'subway_station', 'aquarium', \n    'museum', 'home_office', 'hospital_room', 'kindergarden_classroom', 'butchers_shop', \n    'candy_store', 'office', 'conference_room', 'assembly_line', 'dorm_room'\n]\n\n# Load the dataset\ncsv_file = '/kaggle/input/images256/files.csv'\nroot_dir = '/kaggle/input/images256/'\ndataset = CustomDataset(csv_file=csv_file, root_dir=root_dir, transform=val_test_transform, indoor_classes=indoor_classes)\n\n# Function to create models\ndef create_model(model_name, num_classes):\n    if model_name == 'resnet50':\n        model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n        model.fc = nn.Linear(model.fc.in_features, num_classes)\n    elif model_name == 'resnet18':\n        model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n        model.fc = nn.Linear(model.fc.in_features, num_classes)\n    elif model_name == 'efficientnet_b0':\n        model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n    elif model_name == 'efficientnet_v2_s':\n        model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n    else:\n        raise ValueError(\"Invalid model name\")\n    return model\n\n# Function to load the models\ndef load_model(model_path, model_name, num_classes):\n    model = create_model(model_name, num_classes)\n    state_dict = torch.load(model_path, map_location=device)\n    \n    # Check if the model was saved with DataParallel\n    if next(iter(state_dict)).startswith('module.'):\n        # Remove the 'module.' prefix\n        state_dict = {k[len('module.'):]: v for k, v in state_dict.items()}\n    \n    # Load the state dict\n    model.load_state_dict(state_dict, strict=False)  # Use strict=False to ignore the fc layer mismatch\n    \n    model = model.to(device)\n    \n    # Wrap the model in DataParallel if multiple GPUs are available\n    if torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs!\")\n        model = nn.DataParallel(model)\n    \n    model.eval()\n    return model\n\n# Function to evaluate the model with additional metrics\ndef evaluate_model(model, test_loader):\n    y_true = []\n    y_pred = []\n    y_probs = []\n    start_time = time.time()\n    cpu_power_usage = 0.0\n    gpu_power_usage = 0.0\n    nvmlInit()\n    gpu_handles = [nvmlDeviceGetHandleByIndex(i) for i in range(torch.cuda.device_count())] if torch.cuda.is_available() else []\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(test_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n            cpu_power_start = psutil.cpu_percent(interval=None)\n            gpu_power_start = sum([nvmlDeviceGetPowerUsage(handle) for handle in gpu_handles]) if gpu_handles else 0\n            \n            outputs = model(inputs)\n            probs = torch.nn.functional.softmax(outputs, dim=1)\n            _, preds = torch.max(outputs, 1)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(preds.cpu().numpy())\n            y_probs.extend(probs.cpu().numpy())\n            \n            cpu_power_end = psutil.cpu_percent(interval=None)\n            gpu_power_end = sum([nvmlDeviceGetPowerUsage(handle) for handle in gpu_handles]) if gpu_handles else 0\n            \n            cpu_power_usage += cpu_power_end - cpu_power_start\n            gpu_power_usage += (gpu_power_end - gpu_power_start) / 1000  # Convert from mW to W\n            \n    total_time = time.time() - start_time\n    n_images = len(y_true)\n    \n    top1_accuracy = accuracy_score(y_true, y_pred)\n    \n    # Calculate top-5 accuracy\n    y_true_np = np.array(y_true)\n    y_probs_np = np.array(y_probs)\n    top5_preds = np.argsort(y_probs_np, axis=1)[:, -5:]\n    top5_correct = np.any(top5_preds == y_true_np[:, None], axis=1)\n    top5_accuracy = np.mean(top5_correct)\n\n    # Calculate additional metrics\n    precision = precision_score(y_true, y_pred, average='weighted')\n    recall = recall_score(y_true, y_pred, average='weighted')\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    roc_auc = roc_auc_score(y_true, y_probs, multi_class='ovr')\n    \n    nvmlShutdown()\n\n    return {\n        'top1_accuracy': top1_accuracy,\n        'top5_accuracy': top5_accuracy,\n        'total_time': total_time,\n        'n_images': n_images,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'roc_auc': roc_auc,\n        'cpu_power_usage': cpu_power_usage / len(test_loader),\n        'gpu_power_usage': gpu_power_usage / len(test_loader)\n    }\n\n# Initialize NVML for GPU power usage\nnvmlInit()\ngpu_handle = nvmlDeviceGetHandleByIndex(0) if torch.cuda.is_available() else None\n\n# Run multiple trials for each model\nnum_trials = 30\nmodel_paths = [\n    '/kaggle/input/resnet-model/indoor_efficientnet_v2_s_epoch_20.pth',\n    '/kaggle/input/resnet-model/indoor_resnet50_epoch_20.pth',\n    '/kaggle/input/resnet-model/indoor_resnet18_epoch_20.pth',\n    '/kaggle/input/resnet-model/indoor_efficientnet_epoch_20.pth'\n]\nmodel_names = ['efficientnet_v2_s', 'resnet50', 'resnet18', 'efficientnet_b0']\n\ntrial_results = []\n\nfor model_path, model_name in zip(model_paths, model_names):\n    efficiencies = []\n    \n    for trial in range(num_trials):\n        # Set a different seed for each trial to vary the dataset splits\n        set_seed(seed + trial)\n        \n        # Split the dataset into training, validation, and test sets for each trial\n        train_size = int(0.7 * len(dataset))\n        val_size = int(0.15 * len(dataset))\n        test_size = len(dataset) - train_size - val_size\n        train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n        val_dataset.dataset.transform = val_test_transform\n        test_dataset.dataset.transform = val_test_transform\n        test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=num_cores, pin_memory=True)\n        \n        model = load_model(model_path, model_name, num_classes=len(indoor_classes))\n        metrics = evaluate_model(model, test_loader)\n        top1_accuracy = metrics['top1_accuracy']\n        top5_accuracy = metrics['top5_accuracy']\n        total_time = metrics['total_time']\n        n_images = metrics['n_images']\n        trial_time = total_time / n_images\n        \n        trial_results.append({\n            'Model': model_name,\n            'Trial': trial + 1,\n            'Top1_Accuracy': top1_accuracy,\n            'Top5_Accuracy': top5_accuracy,\n            'Time_Per_Image': trial_time,\n            'Precision': metrics['precision'],\n            'Recall': metrics['recall'],\n            'F1': metrics['f1'],\n            'ROC_AUC': metrics['roc_auc'],\n            'CPU Power Usage (W)': metrics['cpu_power_usage'],\n            'GPU Power Usage (W)': metrics['gpu_power_usage']\n        })\n        \n        efficiencies.append(top1_accuracy / trial_time)  # Store efficiency for the model\n\n# Convert trial results to DataFrame\ntrial_results_df = pd.DataFrame(trial_results)\n\n# Calculate averages across all trials for each model\naverage_results = trial_results_df.groupby('Model').mean().reset_index()\naverage_results = average_results.drop(columns=['Trial'])\n\nprint(average_results)\n\n# Save results to CSV files\ntrial_results_df.to_csv('/kaggle/working/indoor_model_evaluation_results.csv', index=False)\naverage_results.to_csv('/kaggle/working/average_indoor_model_evaluation_results.csv', index=False)\n\n# Shutdown NVML\nnvmlShutdown()","metadata":{"execution":{"iopub.status.busy":"2024-06-18T19:25:02.158731Z","iopub.execute_input":"2024-06-18T19:25:02.159062Z","iopub.status.idle":"2024-06-18T20:15:55.172353Z","shell.execute_reply.started":"2024-06-18T19:25:02.159031Z","shell.execute_reply":"2024-06-18T20:15:55.170661Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/efficientnet_v2_s-dd5fe13b.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_v2_s-dd5fe13b.pth\n100%|██████████| 82.7M/82.7M [00:00<00:00, 153MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:37<00:00,  2.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:33<00:00,  2.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:32<00:00,  2.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:33<00:00,  2.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:32<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:32<00:00,  2.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:32<00:00,  2.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:33<00:00,  2.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:33<00:00,  2.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:32<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:32<00:00,  2.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:32<00:00,  2.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:32<00:00,  2.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:32<00:00,  2.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:31<00:00,  3.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:31<00:00,  3.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:31<00:00,  3.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:31<00:00,  3.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:31<00:00,  3.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:31<00:00,  3.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:31<00:00,  3.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:31<00:00,  3.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:31<00:00,  3.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:31<00:00,  3.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:31<00:00,  3.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:31<00:00,  3.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:31<00:00,  3.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:31<00:00,  3.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:31<00:00,  3.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:31<00:00,  3.05it/s]\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 156MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:24<00:00,  3.92it/s]\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 144MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:17<00:00,  5.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:17<00:00,  5.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:17<00:00,  5.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:18<00:00,  5.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:17<00:00,  5.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:18<00:00,  5.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:18<00:00,  5.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:17<00:00,  5.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:18<00:00,  5.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:17<00:00,  5.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:18<00:00,  5.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:18<00:00,  5.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:17<00:00,  5.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:18<00:00,  5.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:17<00:00,  5.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:18<00:00,  5.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:18<00:00,  5.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:17<00:00,  5.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:18<00:00,  5.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:17<00:00,  5.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:18<00:00,  5.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:17<00:00,  5.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:17<00:00,  5.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:18<00:00,  5.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:17<00:00,  5.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:18<00:00,  5.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:17<00:00,  5.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:18<00:00,  5.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:18<00:00,  5.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:17<00:00,  5.44it/s]\nDownloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n100%|██████████| 20.5M/20.5M [00:00<00:00, 109MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:22<00:00,  4.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:22<00:00,  4.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:22<00:00,  4.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:23<00:00,  4.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:23<00:00,  4.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:23<00:00,  4.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:22<00:00,  4.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:23<00:00,  4.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:23<00:00,  4.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:22<00:00,  4.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:22<00:00,  4.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:22<00:00,  4.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:22<00:00,  4.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:23<00:00,  4.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:22<00:00,  4.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:23<00:00,  4.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:23<00:00,  4.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:22<00:00,  4.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:22<00:00,  4.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:22<00:00,  4.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:22<00:00,  4.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:23<00:00,  4.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:22<00:00,  4.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:22<00:00,  4.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:23<00:00,  4.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:22<00:00,  4.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:22<00:00,  4.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:22<00:00,  4.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:22<00:00,  4.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:22<00:00,  4.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"               Model  Top1_Accuracy  Top5_Accuracy  Time_Per_Image  Precision  \\\n0    efficientnet_b0       0.680588       0.930123        0.001875   0.678526   \n1  efficientnet_v2_s       0.836912       0.961211        0.002645   0.837800   \n2           resnet18       0.837912       0.941498        0.001469   0.838911   \n3           resnet50       0.850811       0.948508        0.002006   0.852013   \n\n     Recall        F1   ROC_AUC  CPU Power Usage (W)  GPU Power Usage (W)  \n0  0.680588  0.675077  0.988052             5.180972             6.293434  \n1  0.836912  0.836274  0.993904             7.518333            10.501517  \n2  0.837912  0.837704  0.992288             2.194549            11.087813  \n3  0.850811  0.850833  0.994101             6.258194            11.369083  \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 279\u001b[0m\n\u001b[1;32m    276\u001b[0m nvmlShutdown()\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# Plot the results\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m    280\u001b[0m plt\u001b[38;5;241m.\u001b[39mbar(average_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m], average_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTop1_Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTop-1 Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    281\u001b[0m plt\u001b[38;5;241m.\u001b[39mbar(average_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m], average_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTop5_Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTop-5 Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"],"ename":"NameError","evalue":"name 'plt' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"### All scenes testing","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport random\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom PIL import Image\nfrom torch.cuda.amp import GradScaler, autocast\nimport multiprocessing\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom scipy.stats import friedmanchisquare\nimport scikit_posthocs as sp\nimport seaborn as sns\nimport psutil\nfrom pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetPowerUsage, nvmlShutdown\n\n# Set a seed for reproducibility in initial data splitting\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed = 42\nset_seed(seed)\n\n# Get the number of available CPU cores\nnum_cores = multiprocessing.cpu_count()\n\n# Set the device to GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Define transformations for the validation and test datasets without data augmentation\nval_test_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Custom dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None):\n        self.annotations = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n        image = Image.open(img_path).convert('RGB')\n        label = int(self.annotations.iloc[index, 1])\n\n        if self.transform:\n            image = self.transform(image)\n\n        return (image, label)\n\n# Load the dataset\ncsv_file = '/kaggle/input/images256/files.csv'\nroot_dir = '/kaggle/input/images256/'\ndataset = CustomDataset(csv_file=csv_file, root_dir=root_dir, transform=val_test_transform)\n\n# Function to create models\ndef create_model(model_name, num_classes):\n    if model_name == 'resnet50':\n        model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n        model.fc = nn.Linear(model.fc.in_features, num_classes)\n    elif model_name == 'resnet18':\n        model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n        model.fc = nn.Linear(model.fc.in_features, num_classes)\n    elif model_name == 'efficientnet_b0':\n        model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n    elif model_name == 'efficientnet_v2_s':\n        model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n    else:\n        raise ValueError(\"Invalid model name\")\n    return model\n\n# Function to load the models\ndef load_model(model_path, model_name):\n    model = create_model(model_name, num_classes=205)\n    state_dict = torch.load(model_path, map_location=device)\n    \n    # Check if the model was saved with DataParallel\n    if next(iter(state_dict)).startswith('module.'):\n        # Remove the 'module.' prefix\n        state_dict = {k[len('module.'):]: v for k, v in state_dict.items()}\n    \n    # Load the state dict\n    model.load_state_dict(state_dict, strict=False)  # Use strict=False to ignore the fc layer mismatch\n    \n    model = model.to(device)\n    \n    # Wrap the model in DataParallel if multiple GPUs are available\n    if torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs!\")\n        model = nn.DataParallel(model)\n    \n    model.eval()\n    return model\n\n# Function to evaluate the model with additional metrics\ndef evaluate_model(model, test_loader):\n    y_true = []\n    y_pred = []\n    y_probs = []\n    start_time = time.time()\n    cpu_power_usage = 0.0\n    gpu_power_usage = 0.0\n    nvmlInit()\n    gpu_handles = [nvmlDeviceGetHandleByIndex(i) for i in range(torch.cuda.device_count())] if torch.cuda.is_available() else []\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(test_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n            cpu_power_start = psutil.cpu_percent(interval=None)\n            gpu_power_start = sum([nvmlDeviceGetPowerUsage(handle) for handle in gpu_handles]) if gpu_handles else 0\n            \n            outputs = model(inputs)\n            probs = torch.nn.functional.softmax(outputs, dim=1)\n            _, preds = torch.max(outputs, 1)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(preds.cpu().numpy())\n            y_probs.extend(probs.cpu().numpy())\n            \n            cpu_power_end = psutil.cpu_percent(interval=None)\n            gpu_power_end = sum([nvmlDeviceGetPowerUsage(handle) for handle in gpu_handles]) if gpu_handles else 0\n            \n            cpu_power_usage += cpu_power_end - cpu_power_start\n            gpu_power_usage += (gpu_power_end - gpu_power_start) / 1000  # Convert from mW to W\n            \n    total_time = time.time() - start_time\n    n_images = len(y_true)\n    \n    top1_accuracy = accuracy_score(y_true, y_pred)\n    \n    # Calculate top-5 accuracy\n    y_true_np = np.array(y_true)\n    y_probs_np = np.array(y_probs)\n    top5_preds = np.argsort(y_probs_np, axis=1)[:, -5:]\n    top5_correct = np.any(top5_preds == y_true_np[:, None], axis=1)\n    top5_accuracy = np.mean(top5_correct)\n\n    # Calculate additional metrics\n    precision = precision_score(y_true, y_pred, average='weighted')\n    recall = recall_score(y_true, y_pred, average='weighted')\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    roc_auc = roc_auc_score(y_true, y_probs, multi_class='ovr')\n    \n    nvmlShutdown()\n\n    return {\n        'top1_accuracy': top1_accuracy,\n        'top5_accuracy': top5_accuracy,\n        'total_time': total_time,\n        'n_images': n_images,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'roc_auc': roc_auc,\n        'cpu_power_usage': cpu_power_usage / len(test_loader),\n        'gpu_power_usage': gpu_power_usage / len(test_loader)\n    }\n\n# Run multiple trials for each model\nnum_trials = 30\nmodel_paths = [\n    '/kaggle/input/resnet-model/efficientnet_v2_s_model_epoch_19.pth',\n    '/kaggle/input/resnet-model/resnet50_model_epoch_19.pth',\n    '/kaggle/input/resnet-model/resnet18_model_epoch_19.pth',\n    '/kaggle/input/resnet-model/efficientnet_model_epoch_19.pth'\n]\nmodel_names = ['efficientnet_v2_s', 'resnet50', 'resnet18', 'efficientnet_b0']\n\ntrial_results = []\n\nfor model_path, model_name in zip(model_paths, model_names):\n    efficiencies = []\n    \n    for trial in range(num_trials):\n        # Set a different seed for each trial to vary the dataset splits\n        set_seed(seed + trial)\n        \n        # Split the dataset into training, validation, and test sets for each trial\n        train_size = int(0.7 * len(dataset))\n        val_size = int(0.15 * len(dataset))\n        test_size = len(dataset) - train_size - val_size\n        train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n        val_dataset.dataset.transform = val_test_transform\n        test_dataset.dataset.transform = val_test_transform\n        test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=num_cores, pin_memory=True)\n        \n        model = load_model(model_path, model_name)\n        metrics = evaluate_model(model, test_loader)\n        top1_accuracy = metrics['top1_accuracy']\n        top5_accuracy = metrics['top5_accuracy']\n        total_time = metrics['total_time']\n        n_images = metrics['n_images']\n        trial_time = total_time / n_images\n        \n        trial_results.append({\n            'Model': model_name,\n            'Trial': trial + 1,\n            'Top1_Accuracy': top1_accuracy,\n            'Top5_Accuracy': top5_accuracy,\n            'Time_Per_Image': trial_time,\n            'Precision': metrics['precision'],\n            'Recall': metrics['recall'],\n            'F1': metrics['f1'],\n            'ROC_AUC': metrics['roc_auc'],\n            'CPU Power Usage (W)': metrics['cpu_power_usage'],\n            'GPU Power Usage (W)': metrics['gpu_power_usage']\n        })\n        \n        efficiencies.append(top1_accuracy / trial_time)  # Store efficiency for the model\n\n# Convert trial results to DataFrame\ntrial_results_df = pd.DataFrame(trial_results)\n\n# Calculate averages across all trials for each model\naverage_results = trial_results_df.groupby('Model').mean().reset_index()\naverage_results = average_results.drop(columns=['Trial'])\n\nprint(average_results)\n\n# Save results to a CSV file\ntrial_results_df.to_csv('model_evaluation_results.csv', index=False)\naverage_results.to_csv('average_model_evaluation_results.csv', index=False)\n\n# Calculate the ratio of top-1 accuracy to average time\naverage_results['Top1_Accuracy/AvgTime'] = average_results['Top1_Accuracy'] / average_results['Time_Per_Image']\n\n# Plot the results\nplt.figure(figsize=(10, 6))\nplt.bar(average_results['Model'], average_results['Top1_Accuracy'], color='b', alpha=0.6, label='Top-1 Accuracy')\nplt.bar(average_results['Model'], average_results['Top5_Accuracy'], color='r', alpha=0.6, label='Top-5 Accuracy')\nplt.xlabel('Model')\nplt.ylabel('Accuracy')\nplt.title('Top-1 and Top-5 Accuracy for Different Models')\nplt.legend()\nplt.savefig('model_accuracies.png')\nplt.show()\n\nplt.figure(figsize=(10, 6))\nplt.bar(average_results['Model'], average_results['Time_Per_Image'], color='g', alpha=0.6)\nplt.xlabel('Model')\nplt.ylabel('Average Time per Image (s)')\nplt.title('Average Time per Image for Different Models')\nplt.savefig('model_times.png')\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-18T20:25:06.551358Z","iopub.execute_input":"2024-06-18T20:25:06.551705Z","iopub.status.idle":"2024-06-18T22:54:07.246984Z","shell.execute_reply.started":"2024-06-18T20:25:06.551678Z","shell.execute_reply":"2024-06-18T22:54:07.245925Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/efficientnet_v2_s-dd5fe13b.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_v2_s-dd5fe13b.pth\n100%|██████████| 82.7M/82.7M [00:00<00:00, 133MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [02:19<00:00,  2.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:58<00:00,  2.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:51<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:39<00:00,  2.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:33<00:00,  3.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:32<00:00,  3.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:31<00:00,  3.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:31<00:00,  3.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:30<00:00,  3.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:29<00:00,  3.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:28<00:00,  3.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:29<00:00,  3.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:29<00:00,  3.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:29<00:00,  3.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:30<00:00,  3.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:30<00:00,  3.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:30<00:00,  3.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:30<00:00,  3.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:31<00:00,  3.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:32<00:00,  3.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:31<00:00,  3.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:29<00:00,  3.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:28<00:00,  3.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:31<00:00,  3.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:33<00:00,  2.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:28<00:00,  3.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:28<00:00,  3.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:28<00:00,  3.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:28<00:00,  3.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:28<00:00,  3.18it/s]\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 153MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:08<00:00,  4.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:08<00:00,  4.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:09<00:00,  4.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:10<00:00,  3.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:09<00:00,  4.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:12<00:00,  3.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:08<00:00,  4.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:07<00:00,  4.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:08<00:00,  4.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:07<00:00,  4.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:08<00:00,  4.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:08<00:00,  4.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:09<00:00,  4.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:08<00:00,  4.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:07<00:00,  4.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:07<00:00,  4.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:10<00:00,  3.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:08<00:00,  4.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:07<00:00,  4.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:07<00:00,  4.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:09<00:00,  4.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:08<00:00,  4.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:07<00:00,  4.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:08<00:00,  4.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:08<00:00,  4.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:07<00:00,  4.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:07<00:00,  4.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:07<00:00,  4.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:07<00:00,  4.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:07<00:00,  4.15it/s]\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 141MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:54<00:00,  5.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:51<00:00,  5.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:52<00:00,  5.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:53<00:00,  5.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:54<00:00,  5.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:53<00:00,  5.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:53<00:00,  5.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:52<00:00,  5.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:54<00:00,  5.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:53<00:00,  5.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:53<00:00,  5.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:54<00:00,  5.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:53<00:00,  5.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:53<00:00,  5.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:53<00:00,  5.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:53<00:00,  5.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:54<00:00,  5.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:54<00:00,  5.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:53<00:00,  5.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:53<00:00,  5.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:53<00:00,  5.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:54<00:00,  5.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:53<00:00,  5.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:53<00:00,  5.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:53<00:00,  5.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:53<00:00,  5.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:54<00:00,  5.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:54<00:00,  5.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:53<00:00,  5.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [00:53<00:00,  5.24it/s]\nDownloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n100%|██████████| 20.5M/20.5M [00:00<00:00, 122MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:07<00:00,  4.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:08<00:00,  4.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:07<00:00,  4.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:06<00:00,  4.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:07<00:00,  4.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:08<00:00,  4.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:08<00:00,  4.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:07<00:00,  4.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:06<00:00,  4.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:08<00:00,  4.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:08<00:00,  4.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:07<00:00,  4.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:07<00:00,  4.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:07<00:00,  4.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:08<00:00,  4.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:05<00:00,  4.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:07<00:00,  4.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:07<00:00,  4.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:09<00:00,  4.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:08<00:00,  4.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:08<00:00,  4.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:08<00:00,  4.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:09<00:00,  4.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:09<00:00,  4.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:08<00:00,  4.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:07<00:00,  4.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:06<00:00,  4.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:07<00:00,  4.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:06<00:00,  4.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [01:07<00:00,  4.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"               Model  Top1_Accuracy  Top5_Accuracy  Time_Per_Image  Precision  \\\n0    efficientnet_b0       0.616162       0.891984        0.001899   0.612408   \n1  efficientnet_v2_s       0.784784       0.941405        0.002629   0.786919   \n2           resnet18       0.802614       0.912022        0.001505   0.804903   \n3           resnet50       0.822709       0.924264        0.001917   0.824504   \n\n     Recall        F1   ROC_AUC  CPU Power Usage (W)  GPU Power Usage (W)  \n0  0.616162  0.608769  0.991912             5.610417             5.951436  \n1  0.784784  0.783641  0.995425             4.481286            11.642575  \n2  0.802614  0.802605  0.992680             2.538321            11.914023  \n3  0.822709  0.822887  0.994564             6.624619            11.196599  \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"# Save results to a CSV file\ntrial_results_df.to_csv('model_evaluation_results.csv', index=False)\naverage_results.to_csv('average_model_evaluation_results.csv', index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}